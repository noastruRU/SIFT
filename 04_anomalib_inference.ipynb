{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "440a9c22",
   "metadata": {},
   "source": [
    "## 1. Setup & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5c81ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from anomalib.models import Patchcore\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# Paths\n",
    "CHECKPOINT_DIR = Path(\"checkpoints\")\n",
    "INFERENCE_INPUT = Path(\"inference_images\")  # Folder for new images\n",
    "INFERENCE_OUTPUT = Path(\"inference_results\")\n",
    "INFERENCE_OUTPUT.mkdir(exist_ok=True)\n",
    "INFERENCE_INPUT.mkdir(exist_ok=True)\n",
    "\n",
    "# Load threshold from evaluation\n",
    "EVAL_RESULTS = Path(\"evaluation_results\") / \"evaluation_results.json\"\n",
    "if EVAL_RESULTS.exists():\n",
    "    with open(EVAL_RESULTS) as f:\n",
    "        eval_data = json.load(f)\n",
    "    THRESHOLD = eval_data['metrics']['optimal_threshold']\n",
    "else:\n",
    "    THRESHOLD = 0.5  # Default if evaluation not run\n",
    "\n",
    "print(f\"Using threshold: {THRESHOLD:.4f}\")\n",
    "\n",
    "# Load model\n",
    "try:\n",
    "    model = Patchcore.load_from_checkpoint(CHECKPOINT_DIR / \"patchcore_trained.ckpt\")\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    print(\"‚úì Model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Make sure to run 02_anomalib_train_patchcore.ipynb first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1638875a",
   "metadata": {},
   "source": [
    "## 2. Define Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adf868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare transform (same as training/evaluation)\n",
    "inference_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def infer_single_image(model, image_path, device, threshold=THRESHOLD):\n",
    "    \"\"\"\n",
    "    Run inference on a single image.\n",
    "    \n",
    "    Returns:\n",
    "        - image: Original PIL image\n",
    "        - anomaly_score: Anomaly score (0-1)\n",
    "        - is_anomaly: Boolean (True if score > threshold)\n",
    "        - confidence: Confidence of prediction\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        img_pil = Image.open(image_path).convert('RGB')\n",
    "        img_tensor = inference_transform(img_pil).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            output = model.predict(img_tensor)\n",
    "        \n",
    "        # Extract anomaly score\n",
    "        if isinstance(output, dict):\n",
    "            anomaly_score = float(output.get('anomaly_score', output.get('score', 0.0)))\n",
    "        else:\n",
    "            anomaly_score = float(output.item()) if isinstance(output, torch.Tensor) else float(output)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        anomaly_score = np.clip(anomaly_score, 0, 1)\n",
    "        \n",
    "        # Determine if anomaly\n",
    "        is_anomaly = anomaly_score > threshold\n",
    "        \n",
    "        # Confidence (distance from threshold)\n",
    "        confidence = abs(anomaly_score - threshold)\n",
    "        \n",
    "        return {\n",
    "            'image': img_pil,\n",
    "            'anomaly_score': anomaly_score,\n",
    "            'is_anomaly': is_anomaly,\n",
    "            'confidence': confidence,\n",
    "            'status': 'ANOMALY' if is_anomaly else 'NORMAL'\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úì Inference function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9bb8b0",
   "metadata": {},
   "source": [
    "## 3. Run Inference on Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80be7968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all images from inference folder\n",
    "image_extensions = ['*.png', '*.jpg', '*.jpeg', '*.bmp']\n",
    "image_files = []\n",
    "\n",
    "for ext in image_extensions:\n",
    "    image_files.extend(INFERENCE_INPUT.glob(ext))\n",
    "\n",
    "if not image_files:\n",
    "    print(f\"‚ö† No images found in {INFERENCE_INPUT}\")\n",
    "    print(f\"Please add images to this folder and rerun this cell.\")\n",
    "else:\n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "    # Run inference\n",
    "    results = []\n",
    "    \n",
    "    for idx, img_path in enumerate(sorted(image_files)):\n",
    "        print(f\"  [{idx+1}/{len(image_files)}] Processing {img_path.name}...\")\n",
    "        result = infer_single_image(model, img_path, DEVICE, THRESHOLD)\n",
    "        \n",
    "        if result:\n",
    "            result['filename'] = img_path.name\n",
    "            results.append(result)\n",
    "    \n",
    "    print(f\"\\n‚úì Inference complete: {len(results)} images processed\")\n",
    "    \n",
    "    # Summary\n",
    "    normal_count = sum(1 for r in results if not r['is_anomaly'])\n",
    "    anomaly_count = sum(1 for r in results if r['is_anomaly'])\n",
    "    \n",
    "    print(f\"\\nüìä Results:\")\n",
    "    print(f\"  Normal: {normal_count}\")\n",
    "    print(f\"  Anomalous: {anomaly_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db777921",
   "metadata": {},
   "source": [
    "## 4. Save Detailed Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea25811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results as JSON\n",
    "results_json = []\n",
    "\n",
    "for result in results:\n",
    "    results_json.append({\n",
    "        'filename': result['filename'],\n",
    "        'anomaly_score': float(result['anomaly_score']),\n",
    "        'status': result['status'],\n",
    "        'confidence': float(result['confidence']),\n",
    "        'threshold': THRESHOLD\n",
    "    })\n",
    "\n",
    "results_json_path = INFERENCE_OUTPUT / \"inference_results.json\"\n",
    "with open(results_json_path, 'w') as f:\n",
    "    json.dump(results_json, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Results saved to: {results_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e567659d",
   "metadata": {},
   "source": [
    "## 5. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top 12 results\n",
    "num_vis = min(12, len(results))\n",
    "n_cols = 4\n",
    "n_rows = (num_vis + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4*n_rows))\n",
    "fig.suptitle('Anomalib Inference Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "if num_vis == 1:\n",
    "    axes = np.array([axes])\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "for idx, result in enumerate(results[:num_vis]):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Display image\n",
    "    ax.imshow(result['image'])\n",
    "    \n",
    "    # Color by status\n",
    "    color = 'red' if result['is_anomaly'] else 'green'\n",
    "    status_text = f\"{result['status']}\\nScore: {result['anomaly_score']:.4f}\\nConf: {result['confidence']:.4f}\"\n",
    "    \n",
    "    ax.set_title(status_text, color=color, fontweight='bold', fontsize=11)\n",
    "    ax.set_xlabel(result['filename'], fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(num_vis, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "vis_path = INFERENCE_OUTPUT / \"inference_visualizations.png\"\n",
    "plt.savefig(vis_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"‚úì Visualizations saved to: {vis_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c539b7df",
   "metadata": {},
   "source": [
    "## 6. Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fcd95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot score distribution\n",
    "scores = [r['anomaly_score'] for r in results]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "ax1.hist(scores, bins=20, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(THRESHOLD, color='red', linestyle='--', linewidth=2, label=f'Threshold: {THRESHOLD:.4f}')\n",
    "ax1.set_xlabel('Anomaly Score')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Anomaly Score Distribution')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Bar plot (per image)\n",
    "ax2.barh(range(len(results)), scores, color=['red' if r['is_anomaly'] else 'green' for r in results])\n",
    "ax2.axvline(THRESHOLD, color='blue', linestyle='--', linewidth=2, label='Threshold')\n",
    "ax2.set_xlabel('Anomaly Score')\n",
    "ax2.set_ylabel('Image Index')\n",
    "ax2.set_title('Anomaly Scores by Image')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "dist_path = INFERENCE_OUTPUT / \"score_distribution.png\"\n",
    "plt.savefig(dist_path, dpi=150)\n",
    "print(f\"‚úì Distribution plot saved to: {dist_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(f\"\\nüìà Score Statistics:\")\n",
    "print(f\"  Mean: {np.mean(scores):.4f}\")\n",
    "print(f\"  Median: {np.median(scores):.4f}\")\n",
    "print(f\"  Min: {np.min(scores):.4f}\")\n",
    "print(f\"  Max: {np.max(scores):.4f}\")\n",
    "print(f\"  Std: {np.std(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec28e0f",
   "metadata": {},
   "source": [
    "## 7. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db9d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INFERENCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: Patchcore (wide_resnet50_2)\")\n",
    "print(f\"Threshold: {THRESHOLD:.4f}\")\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"  Total images: {len(results)}\")\n",
    "print(f\"  Normal: {sum(1 for r in results if not r['is_anomaly'])}\")\n",
    "print(f\"  Anomalous: {sum(1 for r in results if r['is_anomaly'])}\")\n",
    "print(f\"\\nüìÅ Output files:\")\n",
    "print(f\"  - Results JSON: {results_json_path}\")\n",
    "print(f\"  - Visualizations: {vis_path}\")\n",
    "print(f\"  - Score distribution: {dist_path}\")\n",
    "print(f\"\\n‚úì Inference complete!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
