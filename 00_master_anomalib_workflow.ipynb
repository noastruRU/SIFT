{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dab4f29",
   "metadata": {},
   "source": [
    "# SECTION 1: SETUP & ENVIRONMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2c35e5",
   "metadata": {},
   "source": [
    "## 1.1 Mount Dataset (For Colab Users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c61d128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Running locally\n",
      "Dataset location: dataset\n"
     ]
    }
   ],
   "source": [
    "# Option 1: If running on Colab, mount Google Drive\n",
    "\n",
    "DATASET_LOCATION = 'dataset'  # Local path\n",
    "print(\"âœ“ Running locally\")\n",
    "\n",
    "print(f\"Dataset location: {DATASET_LOCATION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71fbbc3",
   "metadata": {},
   "source": [
    "## 1.2 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e4ffa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages...\n",
      "  âœ“ torch\n",
      "  âœ“ torch\n",
      "  âœ“ torchvision\n",
      "  âœ“ torchvision\n",
      "  âœ“ pytorch-lightning\n",
      "  âœ“ pytorch-lightning\n",
      "  âœ“ anomalib\n",
      "  âœ“ anomalib\n",
      "  âœ“ opencv-python\n",
      "  âœ“ opencv-python\n",
      "  âœ“ numpy\n",
      "  âœ“ numpy\n",
      "  âœ“ pillow\n",
      "  âœ“ pillow\n",
      "  âœ“ matplotlib\n",
      "  âœ“ matplotlib\n",
      "  âœ“ scikit-learn\n",
      "\n",
      "âœ“ All packages installed!\n",
      "  âœ“ scikit-learn\n",
      "\n",
      "âœ“ All packages installed!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install required packages\n",
    "packages = [\n",
    "    'torch',\n",
    "    'torchvision',\n",
    "    'pytorch-lightning',\n",
    "    'anomalib',\n",
    "    'opencv-python',\n",
    "    'numpy',\n",
    "    'pillow',\n",
    "    'matplotlib',\n",
    "    'scikit-learn'\n",
    "]\n",
    "\n",
    "print(\"Installing packages...\")\n",
    "for package in packages:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "        print(f\"  âœ“ {package}\")\n",
    "    except:\n",
    "        print(f\"  âš  {package} (may already be installed)\")\n",
    "\n",
    "print(\"\\nâœ“ All packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc41f0b",
   "metadata": {},
   "source": [
    "## 1.3 Import Libraries & Set Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0bd6df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Device: cpu\n",
      "\n",
      "âœ“ Paths configured:\n",
      "  Dataset: /content/dataset\n",
      "  Checkpoints: /content/checkpoints\n",
      "  Evaluation: /content/evaluation_results\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix\n",
    "from torchvision import transforms\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import Anomalib\n",
    "from anomalib.models import Patchcore\n",
    "from anomalib.data import Folder\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"âœ“ Device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Set paths\n",
    "DATASET_ROOT = Path(DATASET_LOCATION)\n",
    "TRAIN_GOOD = DATASET_ROOT / \"train\" / \"good\"\n",
    "TEST_GOOD = DATASET_ROOT / \"test\" / \"good\"\n",
    "TEST_DEFECT = DATASET_ROOT / \"test\" / \"defect\"\n",
    "CHECKPOINT_DIR = Path(\"checkpoints\")\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
    "EVALUATION_DIR = Path(\"evaluation_results\")\n",
    "EVALUATION_DIR.mkdir(exist_ok=True)\n",
    "INFERENCE_INPUT = Path(\"inference_images\")\n",
    "INFERENCE_INPUT.mkdir(exist_ok=True)\n",
    "INFERENCE_OUTPUT = Path(\"inference_results\")\n",
    "INFERENCE_OUTPUT.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\\nâœ“ Paths configured:\")\n",
    "print(f\"  Dataset: {DATASET_ROOT.absolute()}\")\n",
    "print(f\"  Checkpoints: {CHECKPOINT_DIR.absolute()}\")\n",
    "print(f\"  Evaluation: {EVALUATION_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465b4795",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 2: DATASET VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab72aac",
   "metadata": {},
   "source": [
    "## 2.1 Validate Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "779a2dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating dataset structure...\n",
      "\n",
      "âœ— Missing: train/good\n",
      "âœ— Missing: test/good\n",
      "âœ— Missing: test/defect\n",
      "\n",
      "==================================================\n",
      "Total training (good): 0\n",
      "Total test (good): 0\n",
      "Total test (defect): 0\n",
      "Total images: 0\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def validate_dataset():\n",
    "    \"\"\"\n",
    "    Validate dataset structure and count images in each folder.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, path in [(\"train/good\", TRAIN_GOOD), \n",
    "                        (\"test/good\", TEST_GOOD), \n",
    "                        (\"test/defect\", TEST_DEFECT)]:\n",
    "        if not path.exists():\n",
    "            print(f\"âœ— Missing: {name}\")\n",
    "            results[name] = 0\n",
    "        else:\n",
    "            images = list(path.glob(\"*.png\")) + list(path.glob(\"*.jpg\")) + list(path.glob(\"*.jpeg\"))\n",
    "            results[name] = len(images)\n",
    "            print(f\"âœ“ {name}: {len(images)} images\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Validating dataset structure...\\n\")\n",
    "counts = validate_dataset()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Total training (good): {counts['train/good']}\")\n",
    "print(f\"Total test (good): {counts['test/good']}\")\n",
    "print(f\"Total test (defect): {counts['test/defect']}\")\n",
    "print(f\"Total images: {sum(counts.values())}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb540b4",
   "metadata": {},
   "source": [
    "## 2.2 Check Image Validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a4842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_image_validity():\n",
    "    \"\"\"\n",
    "    Verify all images can be loaded and check sizes.\n",
    "    \"\"\"\n",
    "    all_sizes = []\n",
    "    invalid_count = 0\n",
    "    \n",
    "    for folder in [TRAIN_GOOD, TEST_GOOD, TEST_DEFECT]:\n",
    "        if not folder.exists():\n",
    "            continue\n",
    "            \n",
    "        images = list(folder.glob(\"*.png\")) + list(folder.glob(\"*.jpg\")) + list(folder.glob(\"*.jpeg\"))\n",
    "        \n",
    "        for img_path in images:\n",
    "            try:\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is None:\n",
    "                    invalid_count += 1\n",
    "                else:\n",
    "                    h, w = img.shape[:2]\n",
    "                    all_sizes.append((w, h))\n",
    "            except Exception as e:\n",
    "                invalid_count += 1\n",
    "    \n",
    "    if all_sizes:\n",
    "        sizes_array = np.array(all_sizes)\n",
    "        print(f\"Image Validity Check:\")\n",
    "        print(f\"  Valid images: {len(all_sizes)}\")\n",
    "        print(f\"  Invalid images: {invalid_count}\")\n",
    "        print(f\"  Width range: {sizes_array[:, 0].min()} - {sizes_array[:, 0].max()}\")\n",
    "        print(f\"  Height range: {sizes_array[:, 1].min()} - {sizes_array[:, 1].max()}\")\n",
    "        print(f\"  Mean size: {sizes_array.mean(axis=0).astype(int)}\")\n",
    "    else:\n",
    "        print(\"âš  No valid images found!\")\n",
    "\n",
    "print(\"Checking image validity...\\n\")\n",
    "check_image_validity()\n",
    "print(\"\\nâœ“ Dataset validation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c38ca97",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 3: TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b838bb48",
   "metadata": {},
   "source": [
    "## 3.1 Setup Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f36078",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up data module...\\n\")\n",
    "\n",
    "# Configure Anomalib Folder datamodule\n",
    "datamodule = Folder(\n",
    "    root=str(DATASET_ROOT),\n",
    "    normal_dir=\"train/good\",  # Train on good images only\n",
    "    abnormal_dir=\"test/defect\",  # Use defects for validation (optional)\n",
    "    task=\"classification\",  # Classification mode (not segmentation)\n",
    "    image_size=224,\n",
    "    batch_size=32,\n",
    "    num_workers=0,  # Set to 0 on Windows/Colab; increase on Linux\n",
    "    train_val_split=0.2,  # 80% train, 20% val from good images\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"âœ“ Data module configured\")\n",
    "print(f\"  - Image size: 224x224\")\n",
    "print(f\"  - Batch size: 32\")\n",
    "print(f\"  - Train/val split: 80/20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d074ed",
   "metadata": {},
   "source": [
    "## 3.2 Initialize & Train Patchcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d166cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing Patchcore model...\\n\")\n",
    "\n",
    "# Initialize Patchcore\n",
    "model = Patchcore(\n",
    "    backbone=\"wide_resnet50_2\",  # Strong backbone\n",
    "    layers=[\"layer2\", \"layer3\"],  # Intermediate layers\n",
    "    num_neighbors=9,\n",
    "    normalization_method=\"min_max\",\n",
    ")\n",
    "\n",
    "print(\"âœ“ Patchcore initialized\")\n",
    "print(f\"  - Backbone: wide_resnet50_2\")\n",
    "print(f\"  - Layers: layer2, layer3\")\n",
    "print(f\"  - Num neighbors: 9\")\n",
    "\n",
    "# Setup trainer\n",
    "print(\"\\nSetting up trainer...\\n\")\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=\"logs\",\n",
    "    name=\"patchcore_training\",\n",
    "    version=\"v1\"\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=CHECKPOINT_DIR,\n",
    "    filename=\"patchcore-{epoch:02d}\",\n",
    "    monitor=\"val_anomaly_map_auroc\",\n",
    "    mode=\"max\",\n",
    "    save_last=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=1,  # Patchcore trains in 1 epoch\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True\n",
    ")\n",
    "\n",
    "print(\"âœ“ Trainer configured\")\n",
    "print(f\"  - Max epochs: 1\")\n",
    "print(f\"  - Accelerator: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d8792f",
   "metadata": {},
   "source": [
    "## 3.3 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923fb355",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Train\n",
    "trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ“ TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save model\n",
    "model_save_path = CHECKPOINT_DIR / \"patchcore_trained.ckpt\"\n",
    "trainer.save_checkpoint(model_save_path)\n",
    "\n",
    "print(f\"\\nâœ“ Model saved to: {model_save_path}\")\n",
    "print(f\"  Size: {os.path.getsize(model_save_path) / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1937f5d1",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 4: EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f43e2e6",
   "metadata": {},
   "source": [
    "## 4.1 Load Model & Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e615360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading trained model for evaluation...\\n\")\n",
    "\n",
    "# Load model\n",
    "model = Patchcore.load_from_checkpoint(CHECKPOINT_DIR / \"patchcore_trained.ckpt\")\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ“ Model loaded\")\n",
    "\n",
    "# Prepare transform\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def load_test_images(folder_path):\n",
    "    images = []\n",
    "    paths = []\n",
    "    for ext in ['*.png', '*.jpg', '*.jpeg']:\n",
    "        for img_path in Path(folder_path).glob(ext):\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_tensor = test_transform(img)\n",
    "                images.append(img_tensor)\n",
    "                paths.append(img_path)\n",
    "            except:\n",
    "                pass\n",
    "    return images, paths\n",
    "\n",
    "# Load test images\n",
    "print(\"Loading test images...\")\n",
    "good_images, good_paths = load_test_images(TEST_GOOD)\n",
    "defect_images, defect_paths = load_test_images(TEST_DEFECT)\n",
    "\n",
    "print(f\"âœ“ Good images: {len(good_images)}\")\n",
    "print(f\"âœ“ Defect images: {len(defect_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73cbfd4",
   "metadata": {},
   "source": [
    "## 4.2 Generate Anomaly Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43c512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_anomaly_score(model, images, device):\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for img in images:\n",
    "            img_batch = img.unsqueeze(0).to(device)\n",
    "            output = model.predict(img_batch)\n",
    "            \n",
    "            if isinstance(output, dict):\n",
    "                score = output.get('anomaly_score', output.get('score', 0.0))\n",
    "            else:\n",
    "                score = output.item() if isinstance(output, torch.Tensor) else float(output)\n",
    "            \n",
    "            scores.append(score)\n",
    "    return np.array(scores)\n",
    "\n",
    "print(\"Generating anomaly scores...\\n\")\n",
    "good_scores = predict_anomaly_score(model, good_images, DEVICE)\n",
    "defect_scores = predict_anomaly_score(model, defect_images, DEVICE)\n",
    "\n",
    "print(f\"Good images:\")\n",
    "print(f\"  Mean: {good_scores.mean():.4f}, Std: {good_scores.std():.4f}\")\n",
    "print(f\"  Min: {good_scores.min():.4f}, Max: {good_scores.max():.4f}\")\n",
    "\n",
    "print(f\"\\nDefect images:\")\n",
    "print(f\"  Mean: {defect_scores.mean():.4f}, Std: {defect_scores.std():.4f}\")\n",
    "print(f\"  Min: {defect_scores.min():.4f}, Max: {defect_scores.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb4348",
   "metadata": {},
   "source": [
    "## 4.3 Calculate Metrics & ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20404ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine labels\n",
    "y_true = np.concatenate([np.zeros(len(good_scores)), np.ones(len(defect_scores))])\n",
    "y_scores = np.concatenate([good_scores, defect_scores])\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Optimal threshold (Youden's index)\n",
    "youden_index = tpr - fpr\n",
    "optimal_idx = np.argmax(youden_index)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "print(f\"\\nAt optimal threshold:\")\n",
    "print(f\"  True Positive Rate: {tpr[optimal_idx]:.4f}\")\n",
    "print(f\"  False Positive Rate: {fpr[optimal_idx]:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "y_pred = (y_scores >= optimal_threshold).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Negatives: {tn}\")\n",
    "print(f\"  False Positives: {fp}\")\n",
    "print(f\"  False Negatives: {fn}\")\n",
    "print(f\"  True Positives: {tp}\")\n",
    "print(f\"\\nSensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc79218e",
   "metadata": {},
   "source": [
    "## 4.4 Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c7565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: ROC Curve\n",
    "axes[0].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "axes[0].scatter(fpr[optimal_idx], tpr[optimal_idx], color='red', s=100, marker='o', label=f'Optimal = {optimal_threshold:.4f}')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve - Patchcore Anomaly Detection')\n",
    "axes[0].legend(loc=\"lower right\")\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Score Distribution\n",
    "axes[1].hist(good_scores, bins=20, alpha=0.7, label='Good', color='green', edgecolor='black')\n",
    "axes[1].hist(defect_scores, bins=20, alpha=0.7, label='Defect', color='red', edgecolor='black')\n",
    "axes[1].axvline(optimal_threshold, color='blue', linestyle='--', linewidth=2, label=f'Threshold = {optimal_threshold:.4f}')\n",
    "axes[1].set_xlabel('Anomaly Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Anomaly Score Distribution')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "roc_path = EVALUATION_DIR / \"roc_and_distribution.png\"\n",
    "plt.savefig(roc_path, dpi=150)\n",
    "print(f\"âœ“ Evaluation plots saved to: {roc_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa3625f",
   "metadata": {},
   "source": [
    "## 4.5 Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results = {\n",
    "    \"model\": \"Patchcore (wide_resnet50_2)\",\n",
    "    \"metrics\": {\n",
    "        \"roc_auc\": float(roc_auc),\n",
    "        \"optimal_threshold\": float(optimal_threshold),\n",
    "        \"sensitivity\": float(sensitivity),\n",
    "        \"specificity\": float(specificity),\n",
    "        \"true_positives\": int(tp),\n",
    "        \"true_negatives\": int(tn),\n",
    "        \"false_positives\": int(fp),\n",
    "        \"false_negatives\": int(fn)\n",
    "    },\n",
    "    \"score_statistics\": {\n",
    "        \"good_mean\": float(good_scores.mean()),\n",
    "        \"good_std\": float(good_scores.std()),\n",
    "        \"defect_mean\": float(defect_scores.mean()),\n",
    "        \"defect_std\": float(defect_scores.std())\n",
    "    },\n",
    "    \"test_set_sizes\": {\n",
    "        \"good_images\": len(good_scores),\n",
    "        \"defect_images\": len(defect_scores)\n",
    "    }\n",
    "}\n",
    "\n",
    "results_path = EVALUATION_DIR / \"evaluation_results.json\"\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3133099e",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 5: INFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb201e3d",
   "metadata": {},
   "source": [
    "## 5.1 Upload Images for Inference (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8e6bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Colab users - upload images for inference\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import files\n",
    "    print(\"Click 'Choose Files' to upload images for inference\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Move uploaded files to inference folder\n",
    "    for filename in uploaded.keys():\n",
    "        import shutil\n",
    "        shutil.move(filename, INFERENCE_INPUT / filename)\n",
    "    \n",
    "    print(f\"\\nâœ“ {len(uploaded)} images uploaded to {INFERENCE_INPUT}\")\n",
    "else:\n",
    "    print(f\"Place images in: {INFERENCE_INPUT.absolute()}\")\n",
    "    print(\"Then run the next cells for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7d6d7c",
   "metadata": {},
   "source": [
    "## 5.2 Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf0fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get inference images\n",
    "image_extensions = ['*.png', '*.jpg', '*.jpeg', '*.bmp']\n",
    "image_files = []\n",
    "\n",
    "for ext in image_extensions:\n",
    "    image_files.extend(INFERENCE_INPUT.glob(ext))\n",
    "\n",
    "if not image_files:\n",
    "    print(f\"âš  No images found in {INFERENCE_INPUT}\")\n",
    "    print(\"Upload images and rerun this cell.\")\n",
    "else:\n",
    "    print(f\"Found {len(image_files)} images for inference\\n\")\n",
    "    \n",
    "    # Inference function\n",
    "    def infer_single_image(model, image_path, device, threshold=optimal_threshold):\n",
    "        try:\n",
    "            img_pil = Image.open(image_path).convert('RGB')\n",
    "            img_tensor = test_transform(img_pil).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = model.predict(img_tensor)\n",
    "            \n",
    "            if isinstance(output, dict):\n",
    "                anomaly_score = float(output.get('anomaly_score', output.get('score', 0.0)))\n",
    "            else:\n",
    "                anomaly_score = float(output.item()) if isinstance(output, torch.Tensor) else float(output)\n",
    "            \n",
    "            anomaly_score = np.clip(anomaly_score, 0, 1)\n",
    "            is_anomaly = anomaly_score > threshold\n",
    "            confidence = abs(anomaly_score - threshold)\n",
    "            \n",
    "            return {\n",
    "                'image': img_pil,\n",
    "                'anomaly_score': anomaly_score,\n",
    "                'is_anomaly': is_anomaly,\n",
    "                'confidence': confidence,\n",
    "                'status': 'ANOMALY' if is_anomaly else 'NORMAL'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # Run inference\n",
    "    results_inference = []\n",
    "    for idx, img_path in enumerate(sorted(image_files)):\n",
    "        print(f\"  [{idx+1}/{len(image_files)}] {img_path.name}...\")\n",
    "        result = infer_single_image(model, img_path, DEVICE, optimal_threshold)\n",
    "        \n",
    "        if result:\n",
    "            result['filename'] = img_path.name\n",
    "            results_inference.append(result)\n",
    "    \n",
    "    print(f\"\\nâœ“ Inference complete: {len(results_inference)} images processed\")\n",
    "    \n",
    "    # Summary\n",
    "    normal_count = sum(1 for r in results_inference if not r['is_anomaly'])\n",
    "    anomaly_count = sum(1 for r in results_inference if r['is_anomaly'])\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Results:\")\n",
    "    print(f\"  Normal: {normal_count}\")\n",
    "    print(f\"  Anomalous: {anomaly_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b0f425",
   "metadata": {},
   "source": [
    "## 5.3 Visualize Inference Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73610eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_inference:\n",
    "    # Visualize\n",
    "    num_vis = min(12, len(results_inference))\n",
    "    n_cols = 4\n",
    "    n_rows = (num_vis + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4*n_rows))\n",
    "    fig.suptitle('Anomalib Inference Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    if num_vis == 1:\n",
    "        axes = np.array([axes])\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for idx, result in enumerate(results_inference[:num_vis]):\n",
    "        ax = axes[idx]\n",
    "        ax.imshow(result['image'])\n",
    "        \n",
    "        color = 'red' if result['is_anomaly'] else 'green'\n",
    "        status_text = f\"{result['status']}\\nScore: {result['anomaly_score']:.4f}\\nConf: {result['confidence']:.4f}\"\n",
    "        \n",
    "        ax.set_title(status_text, color=color, fontweight='bold', fontsize=11)\n",
    "        ax.set_xlabel(result['filename'], fontsize=9)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    for idx in range(num_vis, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    vis_path = INFERENCE_OUTPUT / \"inference_visualizations.png\"\n",
    "    plt.savefig(vis_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"âœ“ Visualizations saved to: {vis_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Save JSON results\n",
    "    results_json = []\n",
    "    for result in results_inference:\n",
    "        results_json.append({\n",
    "            'filename': result['filename'],\n",
    "            'anomaly_score': float(result['anomaly_score']),\n",
    "            'status': result['status'],\n",
    "            'confidence': float(result['confidence']),\n",
    "            'threshold': float(optimal_threshold)\n",
    "        })\n",
    "    \n",
    "    results_json_path = INFERENCE_OUTPUT / \"inference_results.json\"\n",
    "    with open(results_json_path, 'w') as f:\n",
    "        json.dump(results_json, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ“ JSON results saved to: {results_json_path}\")\n",
    "else:\n",
    "    print(\"No inference results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27caf95a",
   "metadata": {},
   "source": [
    "---\n",
    "# SECTION 6: DOWNLOAD RESULTS (Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d345d2",
   "metadata": {},
   "source": [
    "## 6.1 Download All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efba6f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import files\n",
    "    import shutil\n",
    "    \n",
    "    print(\"Preparing files for download...\\n\")\n",
    "    \n",
    "    # Create zip with all results\n",
    "    shutil.make_archive('anomalib_results', 'zip', '.', \n",
    "                       base_dir=['checkpoints', 'evaluation_results', 'inference_results', 'logs'])\n",
    "    \n",
    "    print(\"Downloading results...\\n\")\n",
    "    files.download('anomalib_results.zip')\n",
    "    \n",
    "    print(\"âœ“ Download started!\")\n",
    "    print(\"\\nYour results include:\")\n",
    "    print(\"  - checkpoints/ (trained model)\")\n",
    "    print(\"  - evaluation_results/ (metrics & plots)\")\n",
    "    print(\"  - inference_results/ (inference outputs)\")\n",
    "    print(\"  - logs/ (training logs)\")\n",
    "else:\n",
    "    print(\"Results are saved locally in:\")\n",
    "    print(f\"  - {CHECKPOINT_DIR.absolute()}\")\n",
    "    print(f\"  - {EVALUATION_DIR.absolute()}\")\n",
    "    print(f\"  - {INFERENCE_OUTPUT.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b6a961",
   "metadata": {},
   "source": [
    "---\n",
    "# FINAL SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd7da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ‰ ANOMALIB PATCHCORE COMPLETE WORKFLOW - FINISHED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ“Š SUMMARY:\")\n",
    "print(f\"\\n1. DATASET:\")\n",
    "print(f\"   Training (good): {counts['train/good']} images\")\n",
    "print(f\"   Testing (good): {counts['test/good']} images\")\n",
    "print(f\"   Testing (defect): {counts['test/defect']} images\")\n",
    "\n",
    "print(f\"\\n2. MODEL:\")\n",
    "print(f\"   Architecture: Patchcore (wide_resnet50_2)\")\n",
    "print(f\"   Backbone: wide_resnet50_2\")\n",
    "print(f\"   Layers: layer2, layer3\")\n",
    "print(f\"   Mode: Unsupervised (trained on good images only)\")\n",
    "\n",
    "print(f\"\\n3. EVALUATION METRICS:\")\n",
    "print(f\"   ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"   Optimal Threshold: {optimal_threshold:.4f}\")\n",
    "print(f\"   Sensitivity: {sensitivity:.4f}\")\n",
    "print(f\"   Specificity: {specificity:.4f}\")\n",
    "\n",
    "print(f\"\\n4. OUTPUT FILES:\")\n",
    "print(f\"   Trained Model: {CHECKPOINT_DIR / 'patchcore_trained.ckpt'}\")\n",
    "print(f\"   Evaluation Plots: {EVALUATION_DIR / 'roc_and_distribution.png'}\")\n",
    "print(f\"   Metrics JSON: {EVALUATION_DIR / 'evaluation_results.json'}\")\n",
    "print(f\"   Inference Results: {INFERENCE_OUTPUT / 'inference_results.json'}\")\n",
    "\n",
    "print(f\"\\nâœ… All workflows complete!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
